[["scrubscore.html", "Chapter 2 SCRUBBING N SCORING 2.1 Navigating this Lesson 2.2 Workflow for Scrubbing and Scoring 2.3 Research Vignette 2.4 On Missing Data 2.5 Data Loss Mechanisms 2.6 Diagnosing Missing Data Mechanisms 2.7 Available Information Analysis 2.8 Working the Problem 2.9 APA Style Results 2.10 Practice Problems 2.11 Bonus Reel:", " Chapter 2 SCRUBBING N SCORING [Screencasted Lecture Link] The focus of this chapter is the process of starting with raw data and preparing it for multivariate analysis. To that end, we will address the conceptual considerations and practical steps in scrubbing and scoring. A twist in this lesson is that we are asking you to contribute to the dataset that serves as the basis for the chapter and the practice problems. In the spirit of open science, this dataset is available to you and others for your own learning. Before continuing, please take 15-20 minutes to take the survey titled, Rate-a-Recent-Course: A ReCentering Psych Stats Exercise. The study is approved by the Institutional Review Board at Seattle Pacific University (SPUIRB# 202102011, no expiration). Details about the study, including an informed consent, are included at the link. 2.1 Navigating this Lesson There is about # hours and ## minutes of lecture. If you work through the materials with me it would be good to add another ## hours. 2.1.1 Learning Objectives Learning objectives from this lecture include the following: Recognize the key components of data loss mechanisms (MCAR, MAR, MNAR), including how to diagnose MCAR. Interpret missingness figures produced by packages such as mice and Amelia. Articulate a workflow for scrubbing and scoring data. Use critical data manipulation functions from dplyr including filter(), select(), and mutate(). Interpret code related to missingness (i.e., is.na, !is.na) and the pipe (%&gt;%) 2.1.2 Planning for Practice Using Parents (2013) AIA approach to managing missing data, scrub and score a raw dataset. Options of graded complexity could incude: Repeating the steps in the chapter; differences will be in the number of people who have completed the survey since the chapter was written. Use the dataset that is the source of the chapter, but score a DIFFERENT SET OF ITEMS  MUST SPECIFY. Begin with raw data to which you have access. 2.1.3 Readings &amp; Resources In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list. Parent, M. C. (2013). Handling item-level missing data: Simpler is just as good. The Counseling Psychologist, 41(4), 568600. https://doi.org/10.1177/0011000012445176 Kline, R. B. (2015). Data preparation and psychometrics review. In Principles and Practice of Structural Equation Modeling, Fourth Edition. Guilford Publications. http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4000663 Grolemund, G., &amp; Wickham, H. (n.d.). 5 Data transformation | R for Data Science. Retrieved March 12, 2020, from https://r4ds.had.co.nz/ Grolemund, G., &amp; Wickham, H. (n.d.). 3 Data visualisation | R for Data Science. Retrieved March 12, 2020, from https://r4ds.had.co.nz/ 2.1.4 Packages If hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. #will install the package if not already installed #if(!require(gplots)){install.packages(&quot;gplots&quot;)} #easy plotting for simple ANOVA #if(!require(tidyverse)){install.packages(&quot;tidyverse&quot;)} #creating new variables and other handy functions #if(!require(psych)){install.packages(&quot;psych&quot;)} #for descriptive statistics and writing them as csv files #if(!require(lsr)){install.packages(&quot;lsr&quot;)} #produces effect sizes #if(!require(pwr)){install.packages(&quot;pwr&quot;)} #estimating sample sizes and power analysis #if(!require(apaTAbles)){install.packages(&quot;apaTables&quot;)} #produces an APA style table for ANOVAs and other models 2.2 Workflow for Scrubbing and Scoring The following is a proposed workflow for preparing data for analysis. Here is a narration of the figure: The workflow begins by importing data into R. Most lessons in this series involve simulated data that are created directly in R. Alternatively, data could be: imported intRavenously through programs such as Qualtrics, exported from programs such as Qualtrics to another program (e.g., .xlxs, .csv) imported in other forms (e.g., .csv,.sps, .sav). Scrubbing data by variable naming, specifying variable characteristics such as factoring, ensuring that included particpiants consented to participation, determining and executing the inclusion and exclusion criteria. Conduct preliminary data diagnostics such as outlier anlaysis assessing for univariate and multivariate analysis making transformations and/or corrections Managing missingness by one of two routes Available information analysis (Parent, 2013) at either the item-level or scale level. The result is a single set of data for analysis. If missingness remains, options include pairwise deletion, listwise deletion, or specifying FIML (when available0). Another option is to use multiple imputation. Multiple imputation at either scale level or item-level 2.3 Research Vignette To provide first-hand experience as both the respondent and analyst for the same set of data, you were asked to complete a survey titled, Rate-a-Recent-Course: A ReCentering Psych Stats Exercise. If you havent yet completed it, please consider doing so, now. The survey is administered in Qualtrics. In the chapter I teach two ways to import Qualtrics data into R. We will then use the data to work through the steps identified in the workflow. 2.4 On Missing Data On the topic of missing data, we follow the traditions in most textbooks. We start by considering data loss mechanisms and options for managing missingness. Although the workflow I recommend is fairly straightforward, the topic is not. Quantitative psychologist have produced volumes of research that supports and refutes all of these issues in detail. An in-deth review of this is found in Enders (enders_applied_2010?) text. 2.5 Data Loss Mechanisms We generally classify missingess in data in three different ways (Kline, 2016; Parent, 2013): Missing completely at random (MCAR) is the ideal case (and often unrealistic in actual data). For variable Y this mean that Missingness is due to a factor(s) completely unrelated to the missing data. Stated another way: Missing observations differ from the observed scores only by chance; that is, whether scores on Y are missing or not missing is unrelated to Y itself The presence versus absence of data on Y is unrelated to all other variables in the dataset. That is, the nonmissing data are just a random sample of scores that the researcher would have analyzed had the data been complete. We might think of it as haphazard missing. A respondent is interrupted, looks up, looks down, and skips an item. A computer glitch causes spotty missingness  unrelated to any particular variable. MCAR is the ideal state because results from it should not be biased as a function of the missingness. Missing at random (MAR) missing data arise from a process that is both measured and predictable in a particular sample. Admittedly the use of random in this term is odd, because, by definition, the missingness is not random. Restated: Missingness on Y is unrelated to Y itself, but missingness is on Y is correlated with other variables in the data set. Example: Men are less likely to respond to questions about mental health than women, but among men, the probability of responding is unrelated to their true mental health status. Kline (2016) indicated that information loss due to MAR is potentially recoverable through imputation where missing scores are replaced by predicted scores. The predicted scores are generated from other variables in the data set that predict missingness on Y. If the strength of that prediction is reasonably strong, then results on Y after imputation may be relatively unbiased. In this sense, the MAR pattern is described as ignorable with regard to potential bias. Two types of variables can be used to predict the missing data variables that are in the prediction equation, and auxiliary variables (i.e., variables in the dataset that are not in the prediction equation). Parent (2013) noted that multiple imputation and expectation maximization have frequently been used to manage missingness in MAR circumstances. Missing not at random (MNAR) is when the presence versus absence of scores on Y depend on Y itself. This is non-ignorable. For example, if a patient drops out of a medical RCT because there are unpleasant side effects from the treatment, this discomfort is not measured, but the data is missing due to a process that is unknown in a particular data set. Results based on complete cases only can be severely biased when the data loss pattern is MNAR. That is, a treatment may look more beneficial than it really is if data from patients who were unable to tolerate the treatment are lost. Parent (2013) described MNAR a little differently  but emphasized that the systematic missingness would be related to a variable outside the datset. Parent provided the example of items written in a manner that may be inappropriate for some participants (e.g., asking women about a relationship with their boyfriend/husband, when the woman might be in same gender relationship). If there were not demographic items that could identify the bias, this would be MNAR. Parent strongly advises researchers to carefully proofread and pilot surveys to avoid MNAR circumstances. Kline (2016) noted that the choice of the method to deal with the incomplete records can make a difference in the results, and should be made carefully. 2.6 Diagnosing Missing Data Mechanisms The bad news is that we never really know (with certainty) the type of missing data mechanism in our data. The following tools can help understand the mechanisms that contribute to missingness. Missing data analyses often includes correlations that could predict missingness. Little and Rubin (little_statistical_2002?) proposed a multivariate statistical test of the MCAR assumption that simultaneously compares complete versus incomplete cases on Y across all other variables. If this comparison is significant, then the MCAR hypothesis is rejected. To restate: we want a non-significant result; and we use the sometimes-backwards-sounding NHST (null hypothesis significance testing) language, MCAR cannot be rejected. MCAR can also be examined through a series of t tests of the cases that have missing scores on Y with cases that have complete records on other variables. Unfortunately, sample sizes contribute to problems with interpretation. With low samples, they are underpowered; in large samples they can flag trivial differences. If MCAR is rejected, we are never sure whether the data loss mechanism is MAR or MNAR. There is no magical statistical fix. Kline (2016) wrote, About the best that can be done is to understand the nature of the underlying data loss pattern and accordingly modify your interpretation of the results (p. 85). 2.7 Available Information Analysis Parent (2013) has created a set of recommendations that help us create a streamlined workflow for managing missing data. Parent concluded that in datasets with (a) low levels of missingness, (b) a reasonable sample size, and (c)adequate internal reliability of measures, that available information anlaysis (AIA), mean substitution, and multiple imputation had similar results. Further, in simulation studies where there was (a) low sample size (n = 50), (b) weak associations among items, and (c) a small number of missing items, AIA was equivalent to multiple imputation. Even in cases where the data conditions were the best (i.e., N = 200, moderate correlations, at least 10 items), even 10% missingness (overall) did not produce notable difference among the methods. That is, means, standard errors, and alphas were similar across the methods (AIA, mean substitution, multiple imputation). AIA is an older method of handling missing data that, as its name suggests, uses the available data for analysis and excludes missing data points only for analyses in which the missing data point would be directly involved. This means In the case of research that uses multiple item scales, and analysis takes place at the scale level AIA is used to generate mean scores for the scale using the available data without substituting or imputing values; This method generally produces a fairly complete set of scale-level data where pairwise deletion (the whole row/case/person is skipped) can be used for many analyses that do not permit missing data: correlations, t-tests, ANOVA FIML can be specified in path analysis (if scale scores are missing) and CFA/SEM (where item-level data is required), and some statistics, such as principal components analysis (an item-level analysis) permit missing data, Of course, the researcher could still impute data, but why Parents(2013) recommendations: Scale scores should be first calculated as a mean (average) not a sum. Why? Calculating a sum from available data will result in automatically lower scores in cases where there is missingness. If a sum is required (i.e., because you want to interpret some clinical level of something), calculate the mean first, do the analyse, then transform the results back into the whole-scale equivalent (multiply the mean by the number of items) for any interpretation (or do some math around the sum-level interpretation so that you can interpret a mean). For R script, do not write the script ([item1 + item2 + item3]/3) because this will return an empty entry for participants missing data (same problem as if you were to use sum). There are several functions for properly computing a mean; I will demo the mean_n() function from sjstats package because it allows us to simultaneously specify the tolerance level (next item). Determine your tolerance for missingness (20% seems to be common, although you could also look for guidance in the test manual/article). Then Run a percent missingness check on the level of analysis (i.e., total score, scale, or subscale) you are using. If you are using a total scale score, then check to see what percent is missing across all the items in the whole scale. In contrast, if you are looking at subscales, run the percent missing at that level. Parent (2013) advised that the tolerance levels should be made mindfully. A four-item scale with one item missing, wont meet the 80% threshold, so it may make sense to set a 75% threshold for this scale. Clearly and concisely detail the level of missingness in papers (Parent, 2013, p. 595). This includes tolerance level for missing data by scale or subscale (e.g., 80% or 75%) the number of missing values out of all data points on that scale for all participants and the maximum by participant (e.g., For Scale X, a total of # missing data points out of ### were observed with no participant missing more than a single point.) verify a manual inspection of missing data for obvious patterns (e.g., abnormally high missing rates for only one or two items). This can be accomplished by requesting frequency output for the items and checking the nonmissing data points for each scale, ensuring there are no abnormal spikes in missingness (looking for MNAR). Curiously, Parent (2013) does not recommend that we run all the diagnostic tests. However, because recent reviewers have required them of me, I will demonstrate a series of them, including MCAR. Reducing missingness starts at the survey design  make sure that all people can answer all items (i.e,. relationship-related items may contain heterosexist assumptionswhich would result in an MNAR circumstance) Very practically speaking, Parents (2013) recommendations follow us through the entire data analysis process. We will use a tolerance of 20%. We will create 2 variables (n_miss and prop_miss) for each of the scales, these will be used two ways. In combination, we will delete individuals who have 100% missing data on these 3 scores (this was an individual decision for this particular dataset; it could be different depending on your analytic approach) We will calculate scale scores only on those with 80% of data present. After calculating the scale scores, we will return to analyzing the missingness, looking at the whole dataset. 2.7.1 Data Simulation Here, we simulate the ANOVA data from the # simulation #to write it to an outfile 2.8 Working the Problem 2.9 APA Style Results All thats left to do is write it up! 2.10 Practice Problems Below are three problems with graded levels of complexity. Worked examples, following the first two suggestions will be provided in an Appendix. Repeating the steps in the chapter; differences will be in the number of people who have completed the survey since the chapter was written. Use the dataset that is the source of the chapter, but score a DIFFERENT SET OF ITEMS  MUST SPECIFY. Begin with raw data to which you have access. 2.10.1 Problem #1: Conduct a one-way ANOVA with moreTalk dependent variable. In their study, Tran and Lee (2014) included an outcome variable where participants rated how much longer they would continue the interaction with their partner compared to their interactions in general. The scale ranged from -2 (much less than average) through 0 (average) to 2 (much more than average). Code for simulated data with moreTalk as the dependent variable is below. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Simulate (or import) and format data 5 _____ 2. Evaluate statistical assumptions 5 _____ 3. Conduct omnibus ANOVA (w effect size) 5 _____ 4. Conduct one set of follow-up tests; narrate your choice 5 _____ 5. Describe approach for managing Type I error 5 _____ 6. APA style results with table(s) and figure 5 _____ 7. Explanation to grader 5 _____ Totals 35 _____ 2.10.2 Problem #2: Play around with this simulation. Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If one-way ANOVA is new to you, perhaps you just change the number in set.seed(2021) from 2021 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go. If you are interested in power, change the sample size to something larger or smaller. If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Simulate (or import) and format data 5 _____ 2. Evaluate statistical assumptions 5 _____ 3. Conduct omnibus ANOVA (w effect size) 5 _____ 4. Conduct one set of follow-up tests; narrate your choice 5 _____ 5. Describe approach for managing Type I error 5 _____ 6. APA style results with table(s) and figure 5 _____ 7. Explanation to grader 5 _____ Totals 35 _____ 2.10.3 Problem #3: Try something entirely new. Either (a) find an article from which you can simulate data or (b) create a research vignette of your own. Specify group n, means, and standard deviations. Simulate the data with these. Be thinking about what it takes in terms of sample size, mean differences, and variability to get a statistically significant omnibus. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Narrate the research vignette, describing the IV and DV 5 _____ 2. Simulate (or import) and format data 5 _____ 3. Evaluate statistical assumptions 5 _____ 4. Conduct omnibus ANOVA (w effect size) 5 _____ 5. Conduct one set of follow-up tests; narrate your choice 5 _____ 6. Describe approach for managing Type I error 5 _____ 7. APA style results with table(s) and figure 5 _____ 8 Explanation to grader 5 _____ Totals 35 _____ 2.11 Bonus Reel: Image of a filmstrip signifying that the what follows is considered to be supplemental Coming soon! sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18362) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.4 magrittr_2.0.1 bookdown_0.21 tools_4.0.4 ## [5] htmltools_0.5.0 rstudioapi_0.13 yaml_2.2.1 stringi_1.5.3 ## [9] rmarkdown_2.6 knitr_1.30 stringr_1.4.0 xfun_0.19 ## [13] digest_0.6.27 rlang_0.4.9 evaluate_0.14 Kline, R. B. (2016). Principles and practice of structural equation modeling (4th ed.). Guilford Publications. http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4000663 Parent, M. C. (2013). Handling item-level missing data: Simpler is just as good. The Counseling Psychologist, 41(4), 568600. https://doi.org/10.1177/0011000012445176 Tran, A. G. T. T., &amp; Lee, R. M. (2014). You speak English well! Asian Americans reactions to an exceptionalizing stereotype. Journal of Counseling Psychology, 61(3), 484490. https://doi.org/10.1037/cou0000034 "]]
