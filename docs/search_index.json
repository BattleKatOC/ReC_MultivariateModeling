[["index.html", "DRAFT: ReCentering Psych Stats: Multivariate Modeling Preface Copyright with Open Access", " DRAFT: ReCentering Psych Stats: Multivariate Modeling Lynette H Bikos, PhD, ABPP Preface If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the purpose teaching my classes and gaining formative feedback from a host of stakeholders. The document was last updated on 21 Mar 2021 To center a variable in regression means to set its value at zero and interpret all other values in relation to this reference point. Regarding race and gender, researchers often center male and White at zero. Further, it is typical that research vignettes in statistics textbooks are similarly seated in a White, Western (frequently U.S.), heteronormative, framework. The purpose of this project is to create a set of open educational resources (OER) appropriate for doctoral and post-doctoral training that contribute to justice, equity, diversity, inclusion (JEDI), and social responsivity. Statistics training in doctoral programs are frequently taught with fee-for-use programs (e.g., SPSS/AMOS, SAS, MPlus) that may not be readily available to the post-doctoral professional. In recent years, there has been an increase and improvement in R packages (e.g., psych, lavaan) used for in analyses common to psychological research. Correspondingly, many graduate programs are transitioning to statistics training in R (free and open source). This is a challenge for post-doctoral psychologists who were trained with other software. This OER will offer statistics training with R and be, freely available on open science platforms under a Creative Commons Attribution - Non Commercial - Share Alike license [CC BY-NC-SA 4.0]. Training models for doctoral programs in HSP are commonly scholar-practitioner, scientist-practitioner, or clinical-scientist. An emerging model, the scientist-practitioner-advocacy training model incorporates social justice advocacy so that graduates are equipped to recognize and address the sociocultural context of oppression and unjust distribution of resources and opportunities (Mallinckrodt et al., 2014). In statistics textbooks, the use of research vignettes engages the learner around a tangible scenario for identifying independent variables, dependent variables, covariates, and potential mechanisms of change. Many students recall examples in Fields (2012) popular statistics text: Viagra to teach one-way ANOVA, beer goggles for two-way ANOVA, and bushtucker for repeated measures. What if the research vignettes were investigations around JEDI/social responsivity and highlighted the scholarship of individuals whose identities are often marginalized? In this OER, research vignettes will be from recently published articles where: (a) the authors identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, emerging nations), (b) the research has a JEDI/social responsivity focus, (b) the lessons statistic is used in the article, and (c) the data is shared publicly or there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s). An image of the book cover. It includes four quadrants of non-normal distributions representing gender, race/ethnicty, sustainability/global concerns, and journal articles Copyright with Open Access This book is published under a a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA. Our GitHub open-source repository contains all of the text and source code for the book, including data and images. "],["ReCintro.html", "Chapter 1 Introduction 1.1 What to expect in each chapter 1.2 If You are New to R 1.3 Maximizing Learning by Accessing all the Resources", " Chapter 1 Introduction 1.1 What to expect in each chapter This textbook is intended as applied, in that a primary goal is to help the scientist-practitioner-advocate use the statistic in a research problem and write it up for a program evaluation, dissertation, or journal article. In support of that goal, I try to provide just enough conceptual information so that the researcher can select the appropriate statistic (i.e., distinguishing between when ANOVA is appropriate and when regression is appropriate) and assign variables to their proper role (e.g., covariate, moderator, mediator). This conceptual approach does include step-by-step hand-calculations (only we calculate them arithmetically in R) to provide a visceral feeling of what is happening within the statistical algorithm that may be invisible to the researcher. Additionally, the conceptual review includes a review of the assumptions about the characteristics of the data and research design that are required for the statistic. Statistics can be daunting, so I have worked hard to establish a workflow through each analysis. When possible, I include a flowchart that is referenced frequently in each chapter and assists the the researcher keep track of their place in the many steps and choices that accompany even the simplest of analyses. As with many statistics texts, each chapter includes a research vignette. Somewhat unique to this resource is that the vignettes are selected from recently published articles. Each vignette is chosen with the intent to meet as many of the following criteria as possible: the statistic that is the focus of the chapter was properly used in the article, the authors identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, emerging nations), the research has a justice, equity, inclusion, diversity, and social responsivity focus and will contribute positively to a social justice pedagogy, and the data is available in a repository or there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s). In each chapter we employ R packages that will efficiently calculate the statistic and the dashboard of metrics (e.g., effect sizes, confidence intervals) that are typically reported in psychological science. 1.2 If You are New to R In this section I will provide some orientation to using R in psychological research. I dont intend this text to be R from the ground up, but I will provide enough guidance that the individual can start working the problems. I will also link to tutorials that help individuals get started in R. 1.2.1 R Hygiene Many problems in R can be resolved with good R hygiene. I will review those practices here. I will also review the conventions I will try to consistently use throughout the text. Everything in a folder Although it is often considered good R etiquette to load all packages at the beginning of the script, this is not my practice. Each chapter will include a script that lists all the packages used, but I will open each package as we use it. This will provide a greater sense of which packages are needed for what task. 1.3 Maximizing Learning by Accessing all the Resources In this section I will talk about the options for using and learning the materials. Primarily these are: Open a new document of R, copying the script over and running it in a fresh document. Forking the entire suite of materials from GitHub and working directly in the original R markdown documents with the option to annotate the material directly. Forking the materials to adopt and or adapt for teaching. Listening to the accompanying lectures (I sound best when the speed is 1.75). I intend to record the lectures in Panopto which provides some closed captioning options. Making use of the practice problems. "],["dataprep.html", "DATA PREP", " DATA PREP "],["scrub.html", "Chapter 2 Scrubbing 2.1 Navigating this Lesson 2.2 Workflow for Scrubbing and Scoring 2.3 Research Vignette 2.4 Working the Problem 2.5 Scrubbing 2.6 Steps Toward the APA Style Write-up 2.7 Practice Problems 2.8 Bonus Track: 2.9 References", " Chapter 2 Scrubbing [Screencasted Lecture Link] The focus of this chapter is the process of starting with raw data and preparing it for multivariate analysis. To that end, we will address the conceptual considerations and practical steps in scrubbing and scoring. A twist in this lesson is that we are asking you to contribute to the dataset that serves as the basis for the chapter and the practice problems. In the spirit of open science, this dataset is available to you and others for your own learning. Before continuing, please take 15-20 minutes to complete the survey titled, Rate-a-Recent-Course: A ReCentering Psych Stats Exercise. The study is approved by the Institutional Review Board at Seattle Pacific University (SPUIRB# 202102011, no expiration). Details about the study, including an informed consent, are included at the link. 2.1 Navigating this Lesson There is about # hours and ## minutes of lecture. If you work through the materials with me it would be good to add another ## hours. 2.1.1 Learning Objectives Learning objectives from this lecture include the following: Import data from Qualtrics into R. Begin the scrubbing process by applying exclusion and inclusion criteria. Rename variables. Create a smaller dataframe with variables appropriate for testing a specific statistical model. Use critical data manipulation functions from the tidyverse (and dplyr) in particular such as filter(), select(), and mutate() to prepare variables. Articulate the initial steps in a workflow for scrubbing and scoring data. 2.1.2 Planning for Practice The suggestions for practice will start with this chapter and continue in the next two chapters (Scoring, Data dx). Using Parents (2013) AIA approach to managing missing data, you will scrub-and-score a raw dataset. Options of graded complexity could incude: Repeating the steps in the chapter with the additional data (it is quite likely that more will have completed the survey) Use the dataset that is the source of the chapter, but select a different set of variables for a model you woud like to test. Begin with raw data to which you have access. 2.1.3 Readings &amp; Resources In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list. Parent, M. C. (2013). Handling item-level missing data: Simpler is just as good. The Counseling Psychologist, 41(4), 568600. https://doi.org/10.1177/0011000012445176 Kline, R. B. (2015). Data preparation and psychometrics review. In Principles and Practice of Structural Equation Modeling, Fourth Edition. Guilford Publications. http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4000663 Grolemund, G., &amp; Wickham, H. (n.d.). 5 Data transformation | R for Data Science. Retrieved March 12, 2020, from https://r4ds.had.co.nz/ 2.1.4 Packages The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. #will install the package if not already installed if(!require(qualtRics)){install.packages(&quot;qualtRics&quot;)} if(!require(tidyverse)){install.packages(&quot;tidyverse&quot;)} if(!require(psych)){install.packages(&quot;psych&quot;)} 2.2 Workflow for Scrubbing and Scoring The following is a proposed workflow for preparing data for analysis. An image of a workflow for scrubbing and scoring data. Here is a narration of the figure: The workflow begins by importing data into R. Most lessons in this series involve simulated data that are created directly in R. Alternatively, data could be: imported intRavenously through programs such as Qualtrics, exported from programs such as Qualtrics to another program (e.g., .xlxs, .csv) imported in other forms (e.g., .csv,.sps, .sav). Scrubbing data by variable naming, specifying variable characteristics such as factoring, ensuring that included particpiants consented to participation, determining and executing the inclusion and exclusion criteria. Conduct preliminary data diagnostics such as outlier anlaysis assessing for univariate and multivariate analysis making transformations and/or corrections Managing missingness by one of two routes Available information analysis (Parent, 2013) at either the item-level or scale level. The result is a single set of data for analysis. If missingness remains, options include pairwise deletion, listwise deletion, or specifying FIML (when available). Another option is to use multiple imputation. Multiple imputation at either scale level or item-level 2.3 Research Vignette To provide first-hand experience as both the respondent and analyst for the same set of data, you were asked to complete a survey titled, Rate-a-Recent-Course: A ReCentering Psych Stats Exercise. If you havent yet completed it, please consider doing so, now. In order to reduce the potential threats to validity by providing background information about the survey, I will wait to describe it until later in the chapter. The survey is administered in Qualtrics. In the chapter I teach two ways to import Qualtrics data into R. We will then use the data to work through the steps identified in the workflow. 2.4 Working the Problem 2.4.1 intRavenous Qualtrics I will demonstrate using a Qualtrics account at my institution, Seattle Pacific University. The only surveys in this account are for the Recentering Psych Stats chapters and lessons. All surveys are designed to not capture personally identifying information. Access credentials for the institutional account, individual users account, and survey are essential for getting the survey items and/or results to export into R. The Qualtrics website provides a tutorial for generating an API token. We need two pieces of information: the root_url and an API token. Log into your respective qualtrics.com account. Select Account Settings Choose Qualtrics IDs from the user name dropdown We need the root_url. This is the first part of the web address for the Qualtrics account. For our institution it is: spupsych.az1.qualtrics.com The API token is in the box labeled, API. If it is empty, select, Generate Token. If you do not have this option, locate the brand administrator for your Qualtrics account. They will need to set up your account so that you have API privileges. BE CAREFUL WITH THE API TOKEN This is the key to your Qualtrics accounts. If you leave it in an .rmd file that you forward to someone else, this key and the base URL gives access to every survey in your account. If you share it, you could be releasing survey data to others that would violate confidentiality promises in an IRB application. If you mistakenly give out your API token you can generate a new one within your Qualtrics account and re-protect all its contents. You do need to change the API key/token if you want to download data from a different Qualtrics account. If your list of surveys generates the wrong set of surveys, restart R, make sure you have the correct API token and try again. #only have to run this ONCE to draw from the same Qualtrics account...but will need to get different token if you are changing between accounts library(qualtRics) #qualtrics_api_credentials(api_key = &quot;mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg&quot;, #base_url = &quot;spupsych.az1.qualtrics.com&quot;, overwrite = TRUE, install = TRUE) all_surveys() generates a dataframe containing information about all the surveys stored on your Qualtrics account. surveys &lt;- all_surveys() #View this as an object (found in the right: Environment). #Get survey id # for the next command #If this is showing you the WRONG list of surveys, you are pulling from the wrong Qualtrics account (i.e., maybe this one instead of your own). Go back and change your API token (it saves your old one). Changing the API likely requires a restart of R. surveys To retrieve the survey, use the fetch_survey() function. #obtained with the survey ID # #&quot;surveyID&quot; should be the ID from above #&quot;verbose&quot; prints messages to the R console #&quot;label&quot;, when TRUE, imports data as text responses; if FALSE prints the data as numerical responses #&quot;convert&quot;, when TRUE, attempts to convert certain question types to teh &quot;proper&quot; data type in R; because I don&#39;t like guessing, I want to set up my own factors. #&quot;force_request&quot;, when TRUE, always downloads the survey from the API instead of from a temporary director (i.e., it always goes to the primary source) # &quot;import_id&quot;, when TRUE includes the unique Qualtrics-assigned ID; since I have provided labels, I want false library(qualtRics) QTRX_df &lt;- fetch_survey(surveyID = &quot;SV_b2cClqAlLGQ6nLU&quot;,useLocalTime = TRUE, verbose = FALSE, label=FALSE, convert=FALSE, force_request = TRUE, import_id = FALSE) ## ## -- Column specification -------------------------------------------------------- ## cols( ## .default = col_double(), ## StartDate = col_datetime(format = &quot;&quot;), ## EndDate = col_datetime(format = &quot;&quot;), ## RecordedDate = col_datetime(format = &quot;&quot;), ## ResponseId = col_character(), ## DistributionChannel = col_character(), ## UserLanguage = col_character(), ## Virtual_2 = col_logical(), ## Virtual_6 = col_logical(), ## `5_iPronouns` = col_logical(), ## `5_iGenderConf` = col_logical(), ## `5_iRace` = col_logical(), ## `5_iUS` = col_logical(), ## `5_iDis` = col_logical(), ## `6_iPronouns` = col_logical(), ## `6_iGenderConf` = col_logical(), ## `6_iRace` = col_logical(), ## `6_iUS` = col_logical(), ## `6_iDis` = col_logical(), ## `7_iPronouns` = col_logical(), ## `7_iGenderConf` = col_logical() ## # ... with 18 more columns ## ) ## i Use `spec()` for the full column specifications. It is possible (and helpful, even) to import Qualtrics data that has been downloaded from Qualtrics as a .csv. I demo this in the Bonus Reel. 2.4.2 About the Rate-a-Recent-Course Survey As a teaching activity for the ReCentering Psych Stats OER, the topic of the survey was selected to be consistent with the overall theme of OER. Specifically, the purpose of this study is to understand the campus climate for students whose identities make them vulnerable to bias and discrimination. These include students who are Black, non-Black students of color, LGBTQ+ students, international students, and students with disabilities. Although the dataset should provide the opportunity to test a number of statistical models, one working hypothesis that framed the study is that the there will be a greater sense of belonging and less bias and discrimination when there is similar representation (of identities that are often marginalized) in the instructional faculty and student body. Termed, structural diversity (Lewis &amp; Shah, 2019) this is likely an oversimplification. In fact, an increase in diverse representation without attention to interacting factors can increase hostility on campus (Hurtado, 2007). Thus, we included the task of rating of a single course relates to the larger campus along the dimensions of belonging and bias/discrimination. For example, if a single class has higher ratings on issues of inclusivity, diversity, and respect, we would expect that sentiment to be echoed in the broader institution. Our design has notable limitations You will likely notice that we ask about demographic characteristics of the instructional staff and classmates in the course rated, but we do not ask about the demographic characteristics of the respondent. In making this decision, we likely lose important information; Iacovino and James (2016) have noted that White students perceive campus more favorably than Black student counterparts. We made this decision to protect the identity of the respondent. As you will see when we download the data, if a faculty member asked an entire class to take the survey, the datestamp and a handful of demographic identifiers could very likely identify a student. In certain circumstances, this might be risky in that private information (i.e., gender nonconformity, disclosure of a disability) or course evaluation data could be related back to the student. Further, the items that ask respondents to guess the identities of the instructional staff and classmates are limited, and contrary to best practices in survey construction that recommend providing the option of a write-in a response. After consulting with a diverse group of stakeholders and subject matter experts (and revising the response options numerous times) I have attempted to center anti-Black racism in the U.S. (Mosley et al., 2020, 2021; Singh, 2020). In fact, the display logic does not present the race items when the course is offered outside the U.S. There are only five options for race: biracial/multiracial, Black, non-Black person(s) of color, White, and I did not notice (intended to capture a color-blind response). One unintended negative consequence of this design is that the response options could contribute to colorism (Adames et al., 2021; Capielo Rosario et al., 2019). Another possibility is that the limited options may erase, or make invisible, other identities. At the time that I am writing the first draft of this chapter, the murder of six Asian American women in Atlanta has just occurred and anti-Asian racism has increased nearly 2000% in the past year (P. Kim, 2021; P. Y. Kim, 2021; STOP AAPI HATE, n.d.). While this survey is intended to assess campus climate as a function of race, it unfortunately does not distinguish between many identities that experience marginalization. In parallel, the items asking respondents to identity characteristics of the instructional staff along dimensions of gender, international status, and disability are large buckets and do not include write-in options. Similarly, there was no intent to cause harm by erasing or making invisible individuals whose identities are better defined by different descriptors. Further, no write-in items were allowed. This was also intentional to prevent potential harm caused by people who could leave inappropriate or harmful comments. 2.4.3 The Codebook In order to scrub-and-score a survey, it is critical to know about its content, scoring directions for scales/subscales, and its design. A more complete description of the survey design elements is (or will be) available in the Recentering Psych Stats: Psychometric OER. The review in this chapter provides just-enough information to allow us to make decisions about which items to retain and how to score them. When they are well-written, information in the IRB application and pre-registration can be helpful in the scrubbing and scoring process. Lets look live at the survey. In Qualtrics it is possible to print a PDF that looks very similar to its presentation when someone is taking it. You can access that static version here. We can export a codebook, that is, a Word (or PDF) version of the survey with all the coding. In Qualtrics the protocol is: Survey/Tools/ImportExport/Export Survey to Word. Then select all the options you want (especially Show Coded Values). A tutorial provided by Qualtrics can be found here. This same process can be used to print the PDF example I used above. It is almost impossible to give this lecture without some reference to Qualtrics and the features used in Qualtrics. An import of raw data from Qualtrics into R can be nightmare in that the Qualtrics-assigned variable names are numbers (e.g., QID1, QID2)  but often out of order because the number is assigned when the question is first created. If the survey is reordered, the numbers get out of sequence. Similarly, values for Likert-type scales can also get out of order if the scale anchors are revised (which is common to do). I recommend providing custom variable names and recode values directly in Qualtrics before exporting them into R. A Qualtrics tutorial for this is provided here. In general, consider these qualities when creating variable names: Brevity: historically, SPSS variable names could be a maximum of 8 characters. Intuitive: although variables can be renamed in R (e.g., for use in charts and tables), it is helpful when the name imported from Qualtrics provides some indication of what the variable is. Systematic: start items in a scale with the same stem, followed by the item number  ITEM1, ITEM2, ITEM3. The Rate-a-Recent-Course survey was written using some special features in Qualtrics. These include Display logic Items that are U.S.-centric are only shown if the respondent is taking a course from an institution in the U.S. is a student in the U.S. Loop and merge Because course may have multiple instructional staff, the information asking about demographic characteristics of the instructors is repeated according to the number input by the respondent Random presentation of the 30 items asking about campus climate for the five groups of students Although this might increase the cognitive load of the survey, this helps spread out missingness for respondents who might tire of the survey and stop early Rank ordering of the institutional level (department, school/faculty, campus/university) to which the respondent feels most connected Looking at the QTRX_df, StartDate thru UserLanguage are metadata created by Qualtrics. The remaining variables and associated value labels are in the codebook. 2.5 Scrubbing With a look at our survey, codebook, and imported data, we now get to the business of scRubbing (deleting those who did not give consent, deleting previews, etc.). This level of scrubbing precedes the more formal detection of outliers. 2.5.1 Tools for Data Transcodmation The next stages will provide some experience transforming data with dplyr from the tidyverse. The tidyverse is a system of packages (i.e,. when you download the tidyverse, you download all its packages/members) for data manipulation, exploration and visualization. The packages in the tidyverse share a common design philosophy. These were mostly developed by Hadley Wickham, but more recently, more designers are contributing to them. Tidyverse packages are intended to make statisticians and data scientists more productive by guiding them through workflows that facilitate communication and result in reproducible work products. Fundamentally, the tidyverse is about the connections between the tools that make the workflow possible. Critical packages in the tidyverse include: dplyr: data manipulation: mutate, select, filter, summarize, arrange ggplot2: extravagant graphing tibble: a tibble is a dataframe that provides the user with more (and less) control over the data. readr: gives access to rectangular data like .csv and tables tidyr: tidy data is where each variable is a column, each observation is a row, each value is a cell (duh). tidyrs contributions are gather(wide to long) and spread(long to wide) as well as separate, extract, unite. purrr: facilitates working with functions and vectors. For example, if you write a function, using purrr may help you replace loops with code that is more efficient and intuitive. The tidyverse is ever-evolving  so check frequently for updates and troubleshooting. A handy cheatsheet for data transformation is found here. 2.5.2 Inclusion and Exclusion Criteria For me, the first pass at scrubbing is to eliminate the obvious. In our case this is includes previews and respondents who did not consent to continue. Previews are the researcher-initiated responses usually designed to proofread or troubleshoot survey problems. There could be other first-pass-deletions, such as selecting response between certain dates. I think these first-pass deletions, especially the ones around consent, are important to do as soon as possible. Otherwise, we might delete some of the variables (e.g., timestamps, consent documentation, preview status) and neglect to delete these cases later in the process. We are here in the workflow: An image of a workflow for scrubbing and scoring data. We can either update the existing df (by using the same object), or creating a new df from the old. Either works. In my early years, I tended to create lots of new objects. As I have gained confidence in myself and in R, Im inclined to update the existing df. Why? Because unless you write the object as an outfile (using the same name for the object as for the filename  which I do not recommend), the object used in R does not change the source of the dat. Therefore, it is easy to correct early code and it keeps the global environment less cluttered. In this particular survey, the majority of respondents will take the survey because they clicked an anonymous link provided by Qualtrics. Another Qualtrics distribution method is e-mail. At the time of this writing, we have not recruited by e-mail, but it is is possible we could do so in the future. What we should not include, though, are previews. These are the times when the researcher is self-piloting the survey to look for errors and to troubleshoot. # the filter command is used when we are making inclusion/exclusion decisions about rows # != means do not include cases with &quot;preview&quot; library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.0.4 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.0 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() QTRX_df &lt;- filter (QTRX_df, DistributionChannel != &quot;preview&quot;) #FYI, another way that doesn&#39;t use tidyverse, but gets the same result #QTRX_df &lt;- QTRX_df[!QTRX_df$DistributionChannel == &quot;preview&quot;,] APA Style, and in particular the Journal Article Reporting Standards (JARS) for quantitative research specify that we should report the frequency or percentages of missing data. We would start our counting after eliminating the previews. # I created an object that lists how many rows/cases remain. # I used inline text below to update the text with the new number attempts &lt;- nrow(QTRX_df) attempts ## [1] 29 CAPTURING RESULTS FOR WRITING IT UP: Data screening suggested that 29 individuals opened the survey link. Next lets filter in only those who consented to take the survey. Because Qualtrics discontinued the survey for everyone who did not consent, we do not have to worry that their data is unintentionally included, but it can be useful to mention the number of non-consenters in the summary of missing data. # == are used QTRX_df &lt;-(filter (QTRX_df, Consent == 1)) consented_attempts &lt;- nrow(QTRX_df) consented_attempts ## [1] 29 CAPTURING RESULTS FOR WRITING IT UP: Data screening suggested that 29 individuals opened the survey link. Of those, 29, granted consent and proceeded into the survey items. In this particular study, the categories used to collect race were U.S.-centric. Thus, they were only shown if the respondent indicated that the course being rated was taught by an institution in the U.S. Therefore, an an additional inclusion criteria for this specific research model should be that the course was taught in the U.S. QTRX_df &lt;-(filter (QTRX_df, USinst == 0)) US_inclusion &lt;- nrow(QTRX_df) US_inclusion ## [1] 29 CAPTURING RESULTS FOR WRITING IT UP: Data screening suggested that 29 individuals opened the survey link. Of those, 29, granted consent and proceeded into the survey items. A further inclusion criteria was that the course was taught in the U.S; 29 met this criteria. 2.5.3 Renaming Variables Even though we renamed the variables in Qualtrics, the loop-and-merge variables were auto-renamed such that they each started with a number. I cannot see how to rename these from inside Qualtrics. A potential problem is that, in R, when variable names start with numbers, they need to be surrounded with single quotation marks. I find it easier to rename them now. I used i to start the variable name to represent instructor. The form of the rename() function is this: df_named &lt;- rename(df_raw, NewName1 = OldName1) library(tidyverse) QTRX_df &lt;- rename(QTRX_df, iRace1 = &#39;1_iRace&#39;, iRace2 = &#39;2_iRace&#39;, iRace3 = &#39;3_iRace&#39;, iRace4 = &#39;4_iRace&#39;, iRace5 = &#39;5_iRace&#39;, iRace6 = &#39;6_iRace&#39;, iRace7 = &#39;7_iRace&#39;, iRace8 = &#39;8_iRace&#39;, iRace9 = &#39;9_iRace&#39;, iRace10 = &#39;10_iRace&#39;) Also in Qualtrics, it was not possible to rename the variable (formatted with sliders) that asked respondents to estimate the proportion of classmates in each race-based category. Using the codebook, we can do this now. I will use cm to precede each variable name to represent classmates. QTRX_df &lt;- rename(QTRX_df, cmBiMulti = Race_10, cmBlack = Race_1, cmNBPoC = Race_7, cmWhite = Race_8, cmUnsure = Race_2) Lets also create an ID variable (different from the lengthy Qualtrics-issued ID) and then move it to the front of the distribution. #library(dplyr) QTRX_df &lt;- QTRX_df %&gt;% mutate(ID = row_number()) #moving the ID number to the first column; requires QTRX_df &lt;- QTRX_df%&gt;%select(ID, everything()) 2.5.4 Downsizing the Dataframe Although researchers may differ in their approach, my tendency is to downsize the df to the variables I will be using in my study. These could include variables in the model, demographic variables, and potentially auxillary variables (i.e,. variables not in the model, but that might be used in the case of multiple imputation). This particular survey did not collect demographic information, so that will not be used. The model that I will demonstrate in this research vignette examines the the respondents perceived campus climate for students who are Black, predicted by the the respondents own campus belonging, and also the structural diversity (Lewis &amp; Shah, 2019) proportions of Black students in the classroom and BIPOC (Black, Indigenous, and people of color) instructional staff. I would like to assess the model by having the instructional staff variable to be the %Black instructional staff. At the time that this lecture is being prepared, there is not sufficient Black representation in the staff to model this. The select() function can let us list the variables we want to retain. #You can use the &quot;:&quot; to include all variables from the first to last variable in any sequence; I could have written this more efficiently. I just like to &quot;see&quot; my scales and clusters of variables. Model_df &lt;-(select (QTRX_df, ID, iRace1, iRace2, iRace3, iRace4, iRace5, iRace6, iRace7, iRace8, iRace9, iRace10, cmBiMulti, cmBlack, cmNBPoC, cmWhite, cmUnsure, Belong_1:Belong_3, Blst_1:Blst_6)) It can be helpful to save outfiles of progress as we go along. Here I save this raw file. write.table(Model_df, file=&quot;BlackStntsModel210318.csv&quot;, sep=&quot;,&quot;, col.names=TRUE, row.names=FALSE) 2.6 Steps Toward the APA Style Write-up 2.6.1 Method/Procedure Data screening suggested that 29 individuals opened the survey link. Of those, 29 granted consent and proceeded to the survey items. A further inclusion criteria was that the course was taught in the U.S; 29 met this criteria. 2.7 Practice Problems Starting with this chapter, the practice problems for this and the next two chapters (i.e., Scoring, Data dx) are connected. Whatever practice option(s) you choose, please (a) use raw data that (b) has some data missing. This second criteria will be important in the subsequent chapters. The three problems below are listed in the order of graded complexity. If you are just getting started, you may wish to start with the first problem. If you are more confident, choose the second or third option. 2.7.1 Problem #1: Rework the Chapter Problem Because the Rate-a-Recent-Course survey remains open, it is quite likely that there will be more participants who have taken the survey since this chapter was last updated. If not  please encourage a peer to take it. Even one additional response will change the results. This practice problem encourages you to rework the chapter, as written, with the updated data from the survey. Assignment Component Points Possible Points Earned 1. Import the data from Qualtrics 5 _____ 2. Exclude all previews 5 _____ 3. Include only those who consented 5 _____ 4. Exclude those whose institutions are outside the U.S. 5 _____ 5. Rename variables 5 _____ 6. Downsize the dataframe to the variables of interest 5 _____ 7. Write up of preliminary results 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 2.7.2 Problem #2: Use the Rate-a-Recent-Course Survey, Choosing Different Variables Before starting this option, choose a minimum of three variables from the Rate-a-Recent-Course survey to include in a simple statistical model. Work through the chapter making decisions that are consistent with the research model you have proposed. There will likely be differences at several points in the process. For example, you may wish to include (not exclude) data where the rated-course was offered by an institution outside the U.S. Different decisions may involve an internet search for the R script you will need as you decide on inclusion and exclusion criteria. Assignment Component Points Possible Points Earned 1. Import the data from Qualtrics 5 _____ 2. Exclude all previews 5 _____ 3. Include only those who consented 5 _____ 4. Other exclusionary/inclusionary criteria? 5 _____ 5. Rename variables 5 _____ 6. Downsize the dataframe to the variables of interest 5 _____ 7. Write up of preliminary results 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 2.7.3 Problem #3: Other data Using raw data for which you have access, use the chapter as a rough guide. Your data will likely have unique characteristics that may involved searching for solutions beyond this chapter/OER. Assignment Component Points Possible Points Earned 1. Import the data 5 _____ 2. Include only those who consented 5 _____ 3. Apply other exclusionary/inclusionary critera 5 _____ 4. Addressing unique concerns 5 _____ 5. Rename variables 5 _____ 6. Downsize the dataframe to the variables of interest 5 _____ 7. Write up of preliminary results 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 2.8 Bonus Track: Image of a filmstrip 2.8.1 Importing data from an exported Qualtrics .csv file The lecture focused on the intRavenous import. It is is also possible to download the Qualtrics data in a variety of formats (e.g., CSV, Excel, SPSS). Since I got started using files with the CSV extension (think Excel lite), that is my preference. In Qualtrics, these are the steps to download the data: Projects/YOURsurvey/Data &amp; Analysis/Export &amp; Import/Export data/CSV/Use numeric values I think that it is critical that to save this file in the same folder as the .rmd file that you will use with the data. R is sensitive to characters used filenames As downloaded, my Qualtrics .csv file had a long name with spaces and symbols that are not allowed. Therore, I gave it a simple, sensible, filename, ReC_Download210319.csv. An idiosyncracy of mine is to datestamp filenames. I use two-digit representations of the year, month, and date so that if the letters preceding the date are the same, the files would alphabetize automatically. library(qualtRics) QTRX_csv &lt;- read_survey(&quot;ReC_Download210319.csv&quot;, strip_html = TRUE, import_id = FALSE, time_zone=NULL, legacy = FALSE) ## ## -- Column specification -------------------------------------------------------- ## cols( ## .default = col_double(), ## StartDate = col_datetime(format = &quot;&quot;), ## EndDate = col_datetime(format = &quot;&quot;), ## RecordedDate = col_datetime(format = &quot;&quot;), ## ResponseId = col_character(), ## DistributionChannel = col_character(), ## UserLanguage = col_character(), ## Virtual = col_number(), ## `5_iPronouns` = col_logical(), ## `5_iGenderConf` = col_logical(), ## `5_iRace` = col_logical(), ## `5_iUS` = col_logical(), ## `5_iDis` = col_logical(), ## `6_iPronouns` = col_logical(), ## `6_iGenderConf` = col_logical(), ## `6_iRace` = col_logical(), ## `6_iUS` = col_logical(), ## `6_iDis` = col_logical(), ## `7_iPronouns` = col_logical(), ## `7_iGenderConf` = col_logical(), ## `7_iRace` = col_logical() ## # ... with 17 more columns ## ) ## i Use `spec()` for the full column specifications. Although minor tweaking may be required, the same script above should be applicable to this version of the data. 2.9 References sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18362) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 ## [5] readr_1.4.0 tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.3 ## [9] tidyverse_1.3.0 qualtRics_3.1.4 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.0 sjlabelled_1.1.7 xfun_0.19 haven_2.3.1 ## [5] colorspace_2.0-0 vctrs_0.3.6 generics_0.1.0 htmltools_0.5.0 ## [9] yaml_2.2.1 rlang_0.4.9 pillar_1.4.7 glue_1.4.2 ## [13] withr_2.3.0 DBI_1.1.0 dbplyr_2.0.0 modelr_0.1.8 ## [17] readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 gtable_0.3.0 ## [21] cellranger_1.1.0 rvest_0.3.6 evaluate_0.14 knitr_1.30 ## [25] curl_4.3 fansi_0.4.1 broom_0.7.3 Rcpp_1.0.5 ## [29] backports_1.2.0 scales_1.1.1 jsonlite_1.7.2 fs_1.5.0 ## [33] hms_0.5.3 digest_0.6.27 stringi_1.5.3 insight_0.11.1 ## [37] bookdown_0.21 grid_4.0.4 cli_2.2.0 tools_4.0.4 ## [41] magrittr_2.0.1 crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.1 ## [45] xml2_1.3.2 reprex_0.3.0 lubridate_1.7.9.2 assertthat_0.2.1 ## [49] rmarkdown_2.6 httr_1.4.2 rstudioapi_0.13 R6_2.5.0 ## [53] compiler_4.0.4 "],["score.html", "Chapter 3 Scoring 3.1 Navigating this Lesson 3.2 Workflow for Scrubbing and Scoring 3.3 Research Vignette 3.4 On Missing Data 3.5 Working the Problem 3.6 Scoring 3.7 Missing Analysis: Scale level 3.8 Revisiting Missing Analysis at the Scale Level 3.9 The APA Style Write-Up 3.10 Results 3.11 Practice Problems 3.12 Bonus Reel: 3.13 References", " Chapter 3 Scoring [Screencasted Lecture Link] The focus of this chapter is to continue the process of scrubbing-and-scoring. We continue with the raw data we downloaded and prepared in teh prior chapter. In this chapter we analyze and manage missingness, score scales/subscales, and represent our work with an APA-style write-up. To that end, we will address the conceptual considerations and practical steps in this process. 3.1 Navigating this Lesson There is about # hours and ## minutes of lecture. If you work through the materials with me it would be good to add another ## hours. 3.1.1 Learning Objectives Learning objectives from this lecture include the following: Recognize the key components of data loss mechanisms (MCAR, MAR, MNAR), including how to diagnose MCAR. Interpret missingness figures produced by packages such as mice and Amelia. Articulate a workflow for scrubbing and scoring data. Use critical data manipulation functions from dplyr including filter(), select(), and mutate() to prepare variables. Interpret code related to missingness (i.e., is.na, !is.na) and the pipe (%&gt;%) 3.1.2 Planning for Practice Using Parents (2013) AIA approach to managing missing data, scrub and score a raw dataset. Options of graded complexity could incude: Repeating the steps in the chapter; differences will be in the number of people who have completed the survey since the chapter was written. Use the dataset that is the source of the chapter, but score a DIFFERENT SET OF ITEMS  MUST SPECIFY. Begin with raw data to which you have access. 3.1.3 Readings &amp; Resources In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list. Parent, M. C. (2013). Handling item-level missing data: Simpler is just as good. The Counseling Psychologist, 41(4), 568600. https://doi.org/10.1177/0011000012445176 Kline, R. B. (2015). Data preparation and psychometrics review. In Principles and Practice of Structural Equation Modeling, Fourth Edition. Guilford Publications. http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4000663 Grolemund, G., &amp; Wickham, H. (n.d.). 5 Data transformation | R for Data Science. Retrieved March 12, 2020, from https://r4ds.had.co.nz/ Grolemund, G., &amp; Wickham, H. (n.d.). 3 Data visualization | R for Data Science. Retrieved March 12, 2020, from https://r4ds.had.co.nz/ 3.1.4 Packages The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. if(!require(tidyverse)){install.packages(&quot;tidyverse&quot;)} if(!require(psych)){install.packages(&quot;psych&quot;)} if(!require(formattable)){install.packages(&quot;formattable&quot;)} if(!require(mice)){install.packages(&quot;mice&quot;)} if(!require(sjstats)){install.packages(&quot;sjstats&quot;)} 3.2 Workflow for Scrubbing and Scoring The following is a proposed workflow for preparing data for analysis. The same workflow guides us through the Scrubbing, Scoring, and Data dx chapters. At this stage we are at preliminary data diagnosis. An image of our stage in the workflow for scrubbing and scoring data. 3.3 Research Vignette The research vignette comes from the survey titled, Rate-a-Recent-Course: A ReCentering Psych Stats Exercise and is explained in the prior chapter. In the prior chapter we conducted super-preliminary scrubbing of variables that will allow us to examine the the respondents perceived campus climate for students who are Black, predicted by the the respondents own campus belonging, and also the structural diversity proportions of Black students in the classroom and the BIPOC instructional staff. At present, I see this as a parallel mediation. That is, the perceived campus climate for Black students will be predicted by the respondents sense of belonging, through the proportion of Black classmates and BIPOC (Black, Indigenous, and people of color)instructional staff. I would like to assess the model by having the instructional staff variable to be the %Black instructional staff. At the time that this lecture is being prepared, there is not sufficient Black representation in the staff to model this. An image of the statistical model for which we are preparing data. First, though, lets take a more conceptual look at issues regarding missing data. Well come back to details of the survey as we work with it. 3.4 On Missing Data On the topic of missing data, we follow the traditions in most textbooks. We start by considering data loss mechanisms and options for managing missingness. Although the workflow I recommend is fairly straightforward, the topic is not. Quantitative psychologist have produced volumes of research that supports and refutes all of these issues in detail. An in-deth review of this is found in Enders (Enders, 2010) text. 3.4.1 Data Loss Mechanisms We generally classify missingess in data in three different ways (Kline, 2016; Parent, 2013): Missing completely at random (MCAR) is the ideal case (and often unrealistic in actual data). For variable Y this mean that Missingness is due to a factor(s) completely unrelated to the missing data. Stated another way: Missing observations differ from the observed scores only by chance; that is, whether scores on Y are missing or not missing is unrelated to Y itself The presence versus absence of data on Y is unrelated to all other variables in the dataset. That is, the nonmissing data are just a random sample of scores that the researcher would have analyzed had the data been complete. We might think of it as haphazard missing. A respondent is interrupted, looks up, looks down, and skips an item. A computer glitch causes spotty missingness  unrelated to any particular variable. MCAR is the ideal state because results from it should not be biased as a function of the missingness. Missing at random (MAR) missing data arise from a process that is both measured and predictable in a particular sample. Admittedly the use of random in this term is odd, because, by definition, the missingness is not random. Restated: Missingness on Y is unrelated to Y itself, but Missingness is on Y is correlated with other variables in the data set. Example: Men are less likely to respond to questions about mental health than women, but among men, the probability of responding is unrelated to their true mental health status. Kline (2016) indicated that information loss due to MAR is potentially recoverable through imputation where missing scores are replaced by predicted scores. The predicted scores are generated from other variables in the data set that predict missingness on Y. If the strength of that prediction is reasonably strong, then results on Y after imputation may be relatively unbiased. In this sense, the MAR pattern is described as ignorable with regard to potential bias. Two types of variables can be used to predict the missing data variables that are in the prediction equation, and auxiliary variables (i.e., variables in the dataset that are not in the prediction equation). Parent (2013) noted that multiple imputation and expectation maximization have frequently been used to manage missingness in MAR circumstances. Missing not at random (MNAR) is when the presence versus absence of scores on Y depend on Y itself. This is non-ignorable. For example, if a patient drops out of a medical RCT because there are unpleasant side effects from the treatment, this discomfort is not measured, but the data is missing due to a process that is unknown in a particular data set. Results based on complete cases only can be severely biased when the data loss pattern is MNAR. That is, a treatment may look more beneficial than it really is if data from patients who were unable to tolerate the treatment are lost. Parent (2013) described MNAR a little differently  but emphasized that the systematic missingness would be related to a variable outside the datset. Parent provided the example of items written in a manner that may be inappropriate for some participants (e.g., asking women about a relationship with their boyfriend/husband, when the woman might be in same gender relationship). If there were not demographic items that could identify the bias, this would be MNAR. Parent strongly advises researchers to carefully proofread and pilot surveys to avoid MNAR circumstances. Kline (2016) noted that the choice of the method to deal with the incomplete records can make a difference in the results, and should be made carefully. 3.4.2 Diagnosing Missing Data Mechanisms The bad news is that we never really know (with certainty) the type of missing data mechanism in our data. The following tools can help understand the mechanisms that contribute to missingness. Missing data analyses often includes correlations that could predict missingness. Little and Rubin (Little &amp; Rubin, 2002) proposed a multivariate statistical test of the MCAR assumption that simultaneously compares complete versus incomplete cases on Y across all other variables. If this comparison is significant, then the MCAR hypothesis is rejected. To restate: we want a non-significant result; and we use the sometimes-backwards-sounding NHST (null hypothesis significance testing) language, MCAR cannot be rejected. MCAR can also be examined through a series of t tests of the cases that have missing scores on Y with cases that have complete records on other variables. Unfortunately, sample sizes contribute to problems with interpretation. With low samples, they are underpowered; in large samples they can flag trivial differences. If MCAR is rejected, we are never sure whether the data loss mechanism is MAR or MNAR. There is no magical statistical fix. Kline (2016) wrote, About the best that can be done is to understand the nature of the underlying data loss pattern and accordingly modify your interpretation of the results (p. 85). 3.4.3 Available Information Analysis Parent (2013) has created a set of recommendations that help us create a streamlined workflow for managing missing data. Parent concluded that in datasets with (a) low levels of missingness, (b) a reasonable sample size, and (c) adequate internal reliability of measures, that available information anlaysis (AIA), mean substitution, and multiple imputation had similar results. Further, in simulation studies where there was (a) low sample size (n = 50), (b) weak associations among items, and (c) a small number of missing items, AIA was equivalent to multiple imputation. Even in cases where the data conditions were the best (i.e., N = 200, moderate correlations, at least 10 items), even 10% missingness (overall) did not produce notable difference among the methods. That is, means, standard errors, and alphas were similar across the methods (AIA, mean substitution, multiple imputation). AIA is an older method of handling missing data that, as its name suggests, uses the available data for analysis and excludes missing data points only for analyses in which the missing data point would be directly involved. This means In the case of research that uses multiple item scales, and analysis takes place at the scale level AIA is used to generate mean scores for the scale using the available data without substituting or imputing values; This method generally produces a fairly complete set of scale-level data where pairwise deletion (the whole row/case/person is skipped) can be used for many analyses that do not permit missing data: correlations, t-tests, ANOVA FIML can be specified in path analysis (if scale scores are missing) and CFA/SEM (where item-level data is required), and some statistics, such as principal components analysis (an item-level analysis) permit missing data, Of course, the researcher could still impute data, but why Parents(2013) recommendations: Scale scores should be first calculated as a mean (average) not a sum. Why? Calculating a sum from available data will result in automatically lower scores in cases where there is missingness. If a sum is required (i.e., because you want to interpret some clinical level of something), calculate the mean first, do the analyse, then transform the results back into the whole-scale equivalent (multiply the mean by the number of items) for any interpretation (or do some math around the sum-level interpretation so that you can interpret a mean). For R script, do not write the script ([item1 + item2 + item3]/3) because this will return an empty entry for participants missing data (same problem as if you were to use sum). There are several functions for properly computing a mean; I will demo the mean_n() function from sjstats package because it allows us to simultaneously specify the tolerance level (next item). Determine your tolerance for missingness (20% seems to be common, although you could also look for guidance in the test manual/article). Then Run a percent missingness check on the level of analysis (i.e., total score, scale, or subscale) you are using. If you are using a total scale score, then check to see what percent is missing across all the items in the whole scale. In contrast, if you are looking at subscales, run the percent missing at that level. Parent (2013) advised that the tolerance levels should be made mindfully. A four-item scale with one item missing, wont meet the 80% threshold, so it may make sense to set a 75% threshold for this scale. Clearly and concisely detail the level of missingness in papers (Parent, 2013, p. 595). This includes tolerance level for missing data by scale or subscale (e.g., 80% or 75%) the number of missing values out of all data points on that scale for all participants and the maximum by participant (e.g., For Scale X, a total of # missing data points out of ### were observed with no participant missing more than a single point.) verify a manual inspection of missing data for obvious patterns (e.g., abnormally high missing rates for only one or two items). This can be accomplished by requesting frequency output for the items and checking the nonmissing data points for each scale, ensuring there are no abnormal spikes in missingness (looking for MNAR). Curiously, Parent (2013) does not recommend that we run all the diagnostic tests. However, because recent reviewers have required them of me, I will demonstrate a series of them, including MCAR. Reducing missingness starts at the survey design  make sure that all people can answer all items (i.e,. relationship-related items may contain heterosexist assumptionswhich would result in an MNAR circumstance) Very practically speaking, Parents (2013) recommendations follow us through the entire data analysis process. We will use a tolerance of 20%. We will create 2 variables (n_miss and prop_miss) for each of the scales, these will be used two ways. In combination, we will delete individuals who have 100% missing data on these 3 scores (this was an individual decision for this particular dataset; it could be different depending on your analytic approach) We will calculate scale scores only on those with 80% of data present. After calculating the scale scores, we will return to analyzing the missingness, looking at the whole dataset. 3.5 Working the Problem In the prior chapter we imported the data from Qualtrics and applied the broadest levels of inclusion (e.g., the course rated was offered from an institution in the U.S., the respondent consented to participation) and exclusion (e.g., the survey was not a preview). We then downsized the survey to include the variables we will use in our statistical model. We then saved the data in a .csv file. Presuming that you are working along with me in an .rmd file, if you have placed that file in the same folder as this .rmd file, the following code should read the data into your environment. I use different names for the object/df in my R environment than I use for the filename that holds the data on my computer. Why? I dont want to accidentally overwrite this precious source of data. scrub_df &lt;- read.csv (&quot;BlackStntsModel210318.csv&quot;, head = TRUE, sep = &quot;,&quot;) str(scrub_df) ## &#39;data.frame&#39;: 29 obs. of 25 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ iRace1 : int 3 3 3 3 1 3 3 3 1 0 ... ## $ iRace2 : int 1 NA 1 1 NA NA 3 NA NA 0 ... ## $ iRace3 : int 3 NA NA 3 NA NA NA NA NA 3 ... ## $ iRace4 : int NA NA NA NA NA NA NA NA NA 3 ... ## $ iRace5 : logi NA NA NA NA NA NA ... ## $ iRace6 : logi NA NA NA NA NA NA ... ## $ iRace7 : logi NA NA NA NA NA NA ... ## $ iRace8 : logi NA NA NA NA NA NA ... ## $ iRace9 : logi NA NA NA NA NA NA ... ## $ iRace10 : logi NA NA NA NA NA NA ... ## $ cmBiMulti: int 0 0 0 2 5 15 0 0 0 7 ... ## $ cmBlack : int 0 5 10 6 5 20 0 0 0 4 ... ## $ cmNBPoC : int 39 10 30 19 10 30 40 5 30 13 ... ## $ cmWhite : int 61 85 60 73 80 35 60 90 70 73 ... ## $ cmUnsure : int 0 0 0 0 0 0 0 5 0 3 ... ## $ Belong_1 : int 6 4 NA 5 4 5 6 7 6 3 ... ## $ Belong_2 : int 6 4 3 3 4 6 6 7 6 3 ... ## $ Belong_3 : int 7 6 NA 2 4 5 5 7 6 3 ... ## $ Blst_1 : int 5 6 NA 2 6 5 5 5 5 3 ... ## $ Blst_2 : int 3 6 5 2 1 1 4 4 3 5 ... ## $ Blst_3 : int 5 2 2 2 1 1 4 3 1 2 ... ## $ Blst_4 : int 2 2 2 2 1 2 4 3 2 3 ... ## $ Blst_5 : int 2 4 NA 2 1 1 4 4 1 3 ... ## $ Blst_6 : int 2 1 2 2 1 2 4 3 2 3 ... Lets think about how the variables in our model should be measured: DV: Campus Climate for Black Students (as perceived by the respondent) mean score of the 6 items on that scale (higher scores indicate a climate characterized by hostility, nonresponsiveness, and stigma) 1 item needs to be reverse-coded this scale was adapted from the LGBT Campus Climate Scale (Szymanski &amp; Bissonette, 2020) IV: Belonging mean score for the 3 items on that scale (higher scores indicate a greater sense of belonging) this scale is taken from the Sense of Belonging subscale from the Perceived Cohesion Scale (Bollen &amp; Hoyle, 1990) Proportion of classmates who are Black a single item Proportion of instructional staff who are BIPOC must be calculated from each of the single items for each instructor Our next step is to conduct a preliminary missing data analysis at the item level, across the dataset we are using. The Campus Climate and Belonging scales are traditional in the sense that they have items that we sum. The variable representing proportion of classmates who are Black is a single item. The variable representing the proportion of instructional staff who are BIPOC must be calculated in a manner that takes into consideration the there may be multiple instructors. The survey allowed a respondent to name up to 10 instructors. Looking at the structure of our data, the iRace(1 thru 10) variables are in int or integer format. This means that they are represented as whole numbers. We need them to be represented as factors. R handles factors represented as words well. Therefore, lets use our codebook to reformat this variable, first from integer to factor and second represented with words instead of numbers. scrub_df[,&#39;iRace1&#39;] &lt;- as.factor(scrub_df[,&#39;iRace1&#39;]) scrub_df[,&#39;iRace2&#39;] &lt;- as.factor(scrub_df[,&#39;iRace2&#39;]) scrub_df[,&#39;iRace3&#39;] &lt;- as.factor(scrub_df[,&#39;iRace3&#39;]) scrub_df[,&#39;iRace4&#39;] &lt;- as.factor(scrub_df[,&#39;iRace4&#39;]) scrub_df[,&#39;iRace5&#39;] &lt;- as.factor(scrub_df[,&#39;iRace5&#39;]) scrub_df[,&#39;iRace6&#39;] &lt;- as.factor(scrub_df[,&#39;iRace6&#39;]) scrub_df[,&#39;iRace7&#39;] &lt;- as.factor(scrub_df[,&#39;iRace7&#39;]) scrub_df[,&#39;iRace8&#39;] &lt;- as.factor(scrub_df[,&#39;iRace8&#39;]) scrub_df[,&#39;iRace9&#39;] &lt;- as.factor(scrub_df[,&#39;iRace9&#39;]) scrub_df[,&#39;iRace10&#39;] &lt;- as.factor(scrub_df[,&#39;iRace10&#39;]) Lets check the structure to see if they are factors. str(scrub_df) ## &#39;data.frame&#39;: 29 obs. of 25 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ iRace1 : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 4 4 4 4 2 4 4 4 2 1 ... ## $ iRace2 : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;3&quot;: 2 NA 2 2 NA NA 3 NA NA 1 ... ## $ iRace3 : Factor w/ 2 levels &quot;1&quot;,&quot;3&quot;: 2 NA NA 2 NA NA NA NA NA 2 ... ## $ iRace4 : Factor w/ 2 levels &quot;2&quot;,&quot;3&quot;: NA NA NA NA NA NA NA NA NA 2 ... ## $ iRace5 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ iRace6 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ iRace7 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ iRace8 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ iRace9 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ iRace10 : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ cmBiMulti: int 0 0 0 2 5 15 0 0 0 7 ... ## $ cmBlack : int 0 5 10 6 5 20 0 0 0 4 ... ## $ cmNBPoC : int 39 10 30 19 10 30 40 5 30 13 ... ## $ cmWhite : int 61 85 60 73 80 35 60 90 70 73 ... ## $ cmUnsure : int 0 0 0 0 0 0 0 5 0 3 ... ## $ Belong_1 : int 6 4 NA 5 4 5 6 7 6 3 ... ## $ Belong_2 : int 6 4 3 3 4 6 6 7 6 3 ... ## $ Belong_3 : int 7 6 NA 2 4 5 5 7 6 3 ... ## $ Blst_1 : int 5 6 NA 2 6 5 5 5 5 3 ... ## $ Blst_2 : int 3 6 5 2 1 1 4 4 3 5 ... ## $ Blst_3 : int 5 2 2 2 1 1 4 3 1 2 ... ## $ Blst_4 : int 2 2 2 2 1 2 4 3 2 3 ... ## $ Blst_5 : int 2 4 NA 2 1 1 4 4 1 3 ... ## $ Blst_6 : int 2 1 2 2 1 2 4 3 2 3 ... They are now factors. Lets recode their representation as integers into representation as labels. We could totally recode each into itself, but I want to visually inspect to make sure it recodes correctly. Earlier I used i to indicate instructor; I will simply changet that to t (teacher). The mutate code below is from dplyr  a member of the tidyverse family. The use of the pipe (%&gt;%) allows us to string together a number of commands into a single operation. Im sure there are ways to do this with concatonated lists, but with copy and paste this was simple enough. library(dplyr) scrub_df &lt;- scrub_df %&gt;% mutate(tRace1 = recode(iRace1, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace2 = recode(iRace2, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace3 = recode(iRace3, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace4 = recode(iRace4, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace5 = recode(iRace5, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace6 = recode(iRace6, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace7 = recode(iRace7, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace8 = recode(iRace8, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace9 = recode(iRace9, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;))%&gt;% mutate(tRace10 = recode(iRace10, &quot;0&quot; = &quot;Black&quot;, &quot;1&quot; = &quot;nBpoc&quot;, &quot;2&quot; = &quot;BiMulti&quot;, &quot;3&quot; = &quot;White&quot;, &quot;4&quot; = &quot;NotNotice&quot;)) Calculating the proportion of the not-White instructional staff could likely be accomplished a number of ways. My searching for solutions resulted in this. Hopefully its a fair balance between intuitive and elegant coding. First, I created code that created a new variable (count.BIPOC) by summing across the tRace1 through tRace10 variables, assigning a count of 1, each time the factor value was Black, nBpoc, or BiMulti scrub_df$count.BIPOC &lt;- apply(scrub_df[c(&quot;tRace1&quot;, &quot;tRace2&quot;, &quot;tRace3&quot;, &quot;tRace4&quot;, &quot;tRace5&quot;, &quot;tRace6&quot;, &quot;tRace7&quot;, &quot;tRace8&quot;, &quot;tRace9&quot;, &quot;tRace10&quot;)], 1, function(x) sum(x %in% c(&quot;Black&quot;, &quot;nBpoc&quot;, &quot;BiMulti&quot;))) Next, I created a variable that counted the number of non-missing values across the tRace1 through tRace10 variables. scrub_df$count.nMiss &lt;- apply(scrub_df[c(&quot;tRace1&quot;, &quot;tRace2&quot;, &quot;tRace3&quot;, &quot;tRace4&quot;, &quot;tRace5&quot;, &quot;tRace6&quot;, &quot;tRace7&quot;, &quot;tRace8&quot;, &quot;tRace9&quot;, &quot;tRace10&quot;)], 1, function(x) sum(!is.na(x))) Now to calculate the proportion of BIPOC instructional faculty for each case. scrub_df$iBIPOC_pr = scrub_df$count.BIPOC/scrub_df$count.nMiss 3.5.1 Missing Data Analysis: Whole df and Item level In understanding missingness across the dataset, I think it is important to analyze and manage it, iteratively. We will start with a view of the whole df-level missingness. Subsequently, and consistent with the available information analysis [AIA; Parent (2013)] approach, we will score the scales and then look again at missingness, using the new information to update our decisions about how to manage it. An image of our stage in the workflow for scrubbing and scoring data. Because we just created a host of new variables in creating the prop_BIPOC variable, lets downsize the df so that the calculations are sensible. scrub_df &lt;-(select (scrub_df, ID, iBIPOC_pr, cmBlack, Belong_1:Belong_3, Blst_1:Blst_6)) With a couple of calculations, we create a proportion of item-level missingness. In this chunk I first calculate the number of missing (nmiss) #Calculating number and proportion of item-level missingness scrub_df$nmiss &lt;- scrub_df%&gt;% select(iBIPOC_pr:Blst_6) %&gt;% #the colon allows us to include all variables between the two listed (the variables need to be in order) is.na %&gt;% rowSums scrub_df&lt;- scrub_df%&gt;% mutate(prop_miss = (nmiss/11)*100) #11 is the number of variables included in calculating the proportion We can grab the descriptives for the prop_miss variable to begin to understand our data. I will create an object from it so I can use it with inline CaseMiss&lt;-psych::describe(scrub_df$prop_miss) CaseMiss ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 29 4.39 10.48 0 2.18 0 0 36.36 36.36 2.27 3.78 1.95 CUMULATIVE CAPTURE FOR WRITING IT UP: Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36.3636364. At the time that I am lecturing this, the the amount of missing is not so aggregious that I want to eliminate any cases. That is, Im willing to wait until after I score the items to make further decisions, then. Because (a) I want to teach it and (b) it is quite likely that we will receive responses with high levels of missingness, I will write code to eliminate cases with \\(\\geq\\) 90%. scrub_df &lt;- filter(scrub_df, prop_miss &lt;= 90) #update df to have only those with at least 90% of complete data To analyze missingness at this level, we need a df that has only the variables of interest. That is, variables like ID and the prop_miss and nmiss variables we created will interfere with an accurate assessment of missingness. I will update our df to eliminate these. scrub_df &lt;- scrub_df %&gt;% select (-c(ID, nmiss, prop_miss))#further update to exclude the n_miss and prop_miss variables Missing data analysis commonly looks at proportions by: the entire df rows/cases/people #install.packages(&quot;formattable&quot;) library(formattable) CellsMiss &lt;- percent(mean(is.na(scrub_df)))#what proportion of cells missing across entire dataset CaseComplete &lt;- percent(mean(complete.cases(scrub_df)))#what proportion of cases (rows) are complete (nonmissing) CellsMiss ## [1] 4.39% CaseComplete ## [1] 79.31% CUMULATIVE CAPTURE FOR WRITING IT UP: Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36.3636364. Across the dataset, 4.39% of cells had missing data and 79.31% of cases had nonmissing data. 3.5.2 Analyzing Missing Data Patterns One approach to analyzing missing data is to assess patterns of missingness. LECTURE? SOMEWHERE? Several R packages are popularly used for conducting such analyse. In the mice package, md.pattern() function provides a matrix with the number of columns + 1, in which each row corresponds to a missing data pattern (1 = observed, 0 = missing). Rows and columns are sorted in increasing amounts of missing information. The last column and row contain row and column counts, respectively. #Using the package: mice library(mice) mice_out &lt;- md.pattern(scrub_df, plot = TRUE, rotate.names = TRUE) mice_out write.csv (mice_out, file=&quot;mice_out.csv&quot;) #optional to write it to a .csv file The table lets you look at each missing pattern and see which variable(s) is/are missing. The output is in the form of a table that indicates the frequency of each pattern of missingness. Because I havent (yet) figured out how to pipe objects from this table into the chapter, this text may dier from the patterns in the current data frame. Each row in the table represents a different pattern of missingness. At the time of writing, there are 5 patterns of missing data. The patterns are listed in descending order of the least amount of missingness. The most common pattern (23 cases, top row) is one with no missing data. One cases (is missing three cells  three of the items assessing the campus climate for Black students, and so forth. In general, the data patterns represented a haphazard patterns of responding (Enders, 2010). 3.5.3 Missing Mechanisms Remember Littles MCAR test. Recently it has a history of appearing, working with glitches, disappearing, and so forth. At the present time I cannot find a package that is working well. When I do, I will add this section. 3.6 Scoring So lets get to work to score up the measures for our analysis. Each step of this should involve careful cross-checking with the codebook. 3.6.1 Reverse scoring As we discovered previously, in the scale that assesses campus climate (higher scores reflect a more negative climate) one of our items (Blst_1, My institution provides a supportive environment for Black students.) requires reverse-coding. To rescore: Create a new variable (this is essential) that is designated as the reversed item. We might put a the letter r (for reverse scoring) at the beginning or end: rBlst_1 or Blst_1r. It does not matter; just be consistent. We dont reverse score into the same variable because when you rerun the script, it just re-reverses the reversed scoreinto infinity. Its very easy to lose your place. The reversal is an equation where you subtract the value in the item from the range/scaling + 1. For the our three items we subtract each items value from 8. scrub_df&lt;- scrub_df %&gt;% mutate(rBlst_1 = 8 - Blst_1) #if you had multiple items, you could add a pipe (%&gt;%) at the end of the line and add more until the last one Per Parent (2013) we will analyze missingness for each scale, separately. We will calculate scale scores on each scale separately when 80% (roughly) of the data is present. this is somewhat arbitrary, on 4 item scales, I would choose 75% (to allow one to be missing) on the 3 item scale, I will allow one item to be missing (65%) After calculating the scale scores, we will return to analyzing the missingness, looking at the whole df The mean_n() function of sjstats package has allows you to specify how many items (whole number) or what percentage of items should be present in order to get the mean. First, though, we should identify the variables (properly formatted, if rescoring was needed) that should be included in the calculation of each scale and subscale. In our case, the scale assessing belonging (Bollen &amp; Hoyle, 1990; Hurtado &amp; Carter, 1997) involves three items with no reversals. Our campus climate scale was adapted from Szymanski et al.s LGBTQ College Campus Climate Scale (Szymanski &amp; Bissonette, 2020). While it has not been psychometrically evaluated for the purpose for which I am using it, I will follow the scoring structure in the journal article that introduces the measure. Specifically, the factor structure permits a total scale score and two subscales representing the college response and stigma. library(sjstats) #Making the list of variables Belonging_vars &lt;- c(&#39;Belong_1&#39;,&#39;Belong_2&#39;,&#39;Belong_3&#39;) ResponseBL_vars &lt;- c(&#39;rBlst_1&#39;, &#39;Blst_4&#39;,&#39;Blst_6&#39;) StigmaBL_vars &lt;- c(&#39;Blst_2&#39;, &#39;Blst_3&#39;,&#39;Blst_5&#39;) ClimateBL_vars &lt;- c(&#39;rBlst_1&#39;, &#39;Blst_4&#39;,&#39;Blst_6&#39;,&#39;Blst_2&#39;, &#39;Blst_3&#39;,&#39;Blst_5&#39; ) #Creating the new variables scrub_df$Belonging &lt;- mean_n(scrub_df[,Belonging_vars], .65) scrub_df$ResponseBL &lt;- mean_n(scrub_df[,ResponseBL_vars], .80) scrub_df$StigmaBL &lt;- mean_n(scrub_df[,StigmaBL_vars], .80) scrub_df$ClimateBL &lt;- mean_n(scrub_df[,ClimateBL_vars], .80) Later it will be helpful to have a df with the item and scale-level variables. Lets save our scrub_df data for this and write it as an outfile. write.table(scrub_df, file=&quot;BlStItmsScrs210320.csv&quot;, sep=&quot;,&quot;, col.names=TRUE, row.names=FALSE) 3.7 Missing Analysis: Scale level Lets return to analyzing the missingness, this time including the scale level variables (without the individual items) that will be in our statistical model(s). An image of our stage in the workflow for scrubbing and scoring data. First lets get the df down to the variables we want to retain: scored &lt;-(select (scrub_df, iBIPOC_pr, cmBlack, Belonging, ResponseBL, StigmaBL, ClimateBL)) ScoredCaseMiss &lt;- nrow(scored) #I produced this object for the sole purpose of feeding the number of cases into the inline text, below CUMULATIVE CAPTURE FOR WRITING IT UP: Across the 29 Before we start our formal analysis of missingness at the scale level, lets continue to scrub by eliminating cases that clearly wont remain. In the script below we create a variable that counts the number of missing variables and then creates a proportion by dividing it by the number of total variables. Using the describe() function from the psych package, we can investigate this variable. #Create a variable (n_miss) that counts the number missing scored$n_miss &lt;- scored%&gt;% select(iBIPOC_pr:ClimateBL) %&gt;% is.na %&gt;% rowSums #Create a proportion missing by dividing n_miss by the total number of variables (80) #Pipe to sort in order of descending frequency to get a sense of the missingness scored&lt;- scored%&gt;% mutate(prop_miss = (n_miss/6)*100)%&gt;% arrange(desc(n_miss)) ScoredPrMiss &lt;- psych::describe(scored$prop_miss) ScoredPrMiss #this object is displayed below and I use input from it for the inline text used in the write-up ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 29 4.6 10.82 0 2.67 0 0 33.33 33.33 1.97 2.29 2.01 CUMULATIVE CAPTURE FOR WRITING IT UP: Across the 29 cases for which the scoring protocol was applied, missingness ranged from 0 to 33.3333333. We need to decide what is our retention threshhold. Twenty percent seems to be a general rule of thumb. Lets delete all cases with missingness at 20% or greater. scored &lt;- filter(scored, prop_miss &lt;= 20) #update df to have only those with at least 20% of complete data (this is an arbitrary decision) scored &lt;-(select (scored, iBIPOC_pr:ClimateBL)) #the variable selection just lops off the proportion missing ScoredCasesIncluded &lt;- nrow(scored) ScoredCasesIncluded #this object is displayed below and I use input from it for the inline text used in the write-up ## [1] 26 CUMULATIVE CAPTURE FOR WRITING IT UP: Across the 29 cases for which the scoring protocol was applied, missingness ranged from zero to 33.3333333. After eliminating cases with greater than 20% missing, the dataset analyzed included 26 cases. With a decision about the number of cases we are going to include, we can continue to analyze missingness. 3.8 Revisiting Missing Analysis at the Scale Level We work with a df that includes only the variables in our model. In our case this is easy. In other cases (i.e., maybe there is an ID number) it might be good to create a subset just for this analysis. Again, we look at missingness as the proportion of individual cells across the dataset, and rows/cases with nonmissing data library(formattable) PrScoredCellsMissing &lt;-percent(mean(is.na(scored))) #percent missing across df PrScoredRowsMissing &lt;- percent(mean(complete.cases(scored))) #percent of rows with nonmissing data PrScoredCellsMissing ## [1] 1.28% PrScoredRowsMissing ## [1] 92.31% CUMULATIVE CAPTURE FOR WRITING IT UP: Across the 29 cases for which the scoring protocol was applied, missingness ranged from zero to 33.3333333. After eliminating cases with greater than 20% missing, the dataset analyzed included 26 cases. In this dataset we had 1.28% missing across the df; 92.31% of the rows had nonmissing data. At this point in the write-up, if I knew I was going to use a consistent approach for managing data (e.g., listwise or pairwise deletion for ordinary least squares regression, FIML for SEM or CFA) I would mention it here. Lets look again at missing patterns and mechanisms. 3.8.1 Scale Level: Patterns of Missing Data Returning to the mice package, we can use the md.pattern() function to examine a matrix with the number of columns + 1 in which each row corresponds to a missing data pattern (1 = observed, 0 = missing). The rows and columns are sorted in increasing amounts of missing information. The last column and row contain row and column counts, respectively. #Using the package: mice library(mice) ## ## Attaching package: &#39;mice&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind mice_ScaleLvl &lt;- md.pattern(scored, plot = TRUE, rotate.names=TRUE) At the scale-level, this is much easier to interpret. As I am lecturing the results today (the numbers will change) there are only two rows of data because there are only two patterns of missingness. The most common pattern is non-missing data (n = 23). The second pattern occurs when two people have missing data on the iBIPOC_pr variable 3.8.2 Is it MCAR? As noted earlier, I cannot find an MCAR package. This is a placeholder for updating this portion of the lecture when one becomes readily available (and I learn about it) 3.8.3 R-eady for Analysis At this stage the data is ready for analysis. With the AIA approach (Parent, 2013) the following preliminary analyses would involve pairwise deletion (i.e., the row/case is dropped for that analysis, but included for all others): An image of our stage in the workflow for scrubbing and scoring data. data diagnostics psychometric properties of scales, such as alpha coefficients assessing assumptions such as univariate and multivariate normality, outliers, etc. preliminary analyses descriptives (means/standard deviations, frequencies) correlation matrices AIA can also be used with primary analyses. Examples of how to manage missingness include: ANOVA/regression models if completed with ordinary least squares, pairwise deletion would be utilized SEM/CFA models with observed, latent, or hybrid models if FIML (well discuss later) is specified, all cases are used, even when there is missingness EFA models these can handle item-level missingness Hierarchical linear modeling/multilevel modeling/mixed effects modeling While all data needs to be present for a given cluster/wave, it is permissible to have varying numbers of clusters/waves per case 3.9 The APA Style Write-Up 3.10 Results All analyses were completed in R Studio (v. 1.4.1106) with R (v. 4.0.4). Missing Data Analysis and Treatment of Missing Data Available item analysis (AIA; (Parent, 2013)) is a strategy for managing missing data that uses available data for analysis and excludes cases with missing data points only for analyses in which the data points would be directly involved. Parent (2013) suggested that AIA is equivalent to more complex methods (e.g., multiple imputation) across a number of variations of sample size, magnitude of associations among items, and degree of missingness. Thus, we utilized Parents recommendations to guide our approach to managing missing data. Missing data analyses were conducted with tools in base R as well as the R packages, psych (v. 1.0.12) and mice (v. 3.13.0). Across cases that were deemed eligible on the basis of the inclusion/exclusion criteria, missingness ranged from 0 to 36.3636364. Across the dataset, 4.39% of cells had missing data and 79.31% of cases had nonmissing data. At this stage in the analysis, we allowed all cases with less than 90% missing to continue to the scoring stage. Guided by Parents (2013) AIA approach, scales with three items were scored if at least two items were non-missing; the scale with 4 items was scored if it at least three non-missig items; and the scale with six items was scored if it had at least five non-missing items. Across the 29 cases for which the scoring protocol was applied, missingness ranged from 0 to 33.3333333. After eliminating cases with greater than 20% missing, the dataset analyzed included ScoredCasesIncluded cases. In this dataset 1.28% was missing across the df; 92.31% of the rows had nonmissing data. Given that our sample sizes were reasonable for the planned analyses and the degree of missingness was low, we specified a . (and here we would say how we analyzed the data: AIA/pairwise deletion, FIML, multiply imputed at the scale levelstay tuned.) 3.11 Practice Problems The three problems described below are designed to be continuations from the previous chapter (Scrubbing). 3.11.1 Problem #1: Reworking the Chapter Problem If you chose this option in the prior chapter, you imported the data from Qualtrics, applied inclusion/exclusion criteria, renamed variables, downsized the df to the variables of interest, and wrote up the preliminary results. Continue working with this data to: Assignment Component 1. Proper formatting of your the variable representing the proportion of BIPOC instructor 5 _____ 2. Proper formatting of your the variable representing the 5 _____ 3. Proper formatting of variables that will be used in the scales/subscales 5 _____ 4. Evaluate and interpret item-level missingness 5 _____ 5. Score any scales/subscales 5 _____ 6. Evaluate and interpret scale-level missingness 5 _____ 7. Represent your work in an APA-style write-up (added to the writeup in the previous chapter) 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 3.11.2 Problem #2: Use the Rate-a-Recent-Course Survey, Choosing Different Variables If you chose this option in the prior chapter, you chose a minimum of three variables from the Rate-a-Recent-Course survey to include in a simple statistical model. You imported the dat from Qualtrics, applied inclusion/exclusion criteria, renamed variables, downsized the df to the variables of interest and wrote up the preliminary results. Continue working with this data to: Assignment Component Points Possible Points Earned 1. Proper formatting of your first variable 5 _____ 2. Proper formatting of your second variable 5 _____ 3. Proper formatting of your third variable 5 _____ 4. Evaluate and interpret item-level missingness 5 _____ 5. Score any scales/subscales 5 _____ 6. Evaluate and interpret scale-level missingness 5 _____ 7. Represent your work in an APA-style write-up (added to the writeup in the previous chapter) 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 3.11.3 Problem #3: Other data If you chose this option in the prior chapter, you used raw data that was available to you. You imported it into R, applied inclusion/exclusion criteria, renamed variables, downsized the df to the variables of interest, and wrote up the preliminary results. Continue working with this data to: Assignment Component Points Possible Points Earned 1. Proper formatting of variables of interest (at least three) 15 _____ 2. Evaluate and interpret item-level missingness 5 _____ 3. Score any scales/subscales 5 _____ 4. Evaluate and interpret scale-level missingness 5 _____ 5. Represent your work in an APA-style write-up (added to the writeup in the previous chapter) 5 _____ 6. Explanation to grader 5 _____ Totals 40 _____ 3.12 Bonus Reel: Image of a filmstrip signifying that the what follows is considered to be supplemental 3.13 References sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18362) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gdtools_0.2.2 mice_3.13.0 sjstats_0.18.0 formattable_0.2.1 ## [5] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 ## [9] readr_1.4.0 tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.3 ## [13] tidyverse_1.3.0 qualtRics_3.1.4 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-151 fs_1.5.0 lubridate_1.7.9.2 insight_0.11.1 ## [5] httr_1.4.2 tools_4.0.4 backports_1.2.0 R6_2.5.0 ## [9] sjlabelled_1.1.7 DBI_1.1.0 colorspace_2.0-0 withr_2.3.0 ## [13] tidyselect_1.1.0 mnormt_2.0.2 emmeans_1.5.3 curl_4.3 ## [17] compiler_4.0.4 performance_0.6.1 cli_2.2.0 rvest_0.3.6 ## [21] xml2_1.3.2 sandwich_3.0-0 bookdown_0.21 bayestestR_0.8.0 ## [25] scales_1.1.1 mvtnorm_1.1-1 psych_2.0.12 systemfonts_0.3.2 ## [29] digest_0.6.27 minqa_1.2.4 svglite_1.2.3.2 rmarkdown_2.6 ## [33] pkgconfig_2.0.3 htmltools_0.5.0 lme4_1.1-26 dbplyr_2.0.0 ## [37] htmlwidgets_1.5.3 rlang_0.4.9 readxl_1.3.1 rstudioapi_0.13 ## [41] generics_0.1.0 zoo_1.8-8 jsonlite_1.7.2 magrittr_2.0.1 ## [45] parameters_0.10.1 Matrix_1.2-18 Rcpp_1.0.5 munsell_0.5.0 ## [49] fansi_0.4.1 lifecycle_0.2.0 stringi_1.5.3 multcomp_1.4-15 ## [53] yaml_2.2.1 MASS_7.3-53 grid_4.0.4 parallel_4.0.4 ## [57] sjmisc_2.8.5 crayon_1.3.4 lattice_0.20-41 haven_2.3.1 ## [61] splines_4.0.4 hms_0.5.3 tmvnsim_1.0-2 knitr_1.30 ## [65] pillar_1.4.7 boot_1.3-25 estimability_1.3 effectsize_0.4.1 ## [69] codetools_0.2-18 reprex_0.3.0 glue_1.4.2 evaluate_0.14 ## [73] modelr_0.1.8 vctrs_0.3.6 nloptr_1.2.2.2 cellranger_1.1.0 ## [77] gtable_0.3.0 assertthat_0.2.1 xfun_0.19 xtable_1.8-4 ## [81] broom_0.7.3 coda_0.19-4 survival_3.2-7 statmod_1.4.35 ## [85] TH.data_1.0-10 ellipsis_0.3.1 Adames, H. Y., Chavez-Dueñas, N. Y., &amp; Jernigan, M. M. (2021). The fallacy of a raceless Latinidad: Action guidelines for centering Blackness in Latinx psychology. Journal of Latinx Psychology, 9(1), 2644. https://doi.org/10.1037/lat0000179 Bollen, K. A., &amp; Hoyle, R. H. (1990). Perceived cohesion: A conceptual and empirical examination. Social Forces, 69(2), 479504. https://doi.org/10.2307/2579670 Capielo Rosario, C., Adames, H. Y., Chavez-Dueñas, N. Y., &amp; Renteria, R. (2019). Acculturation Profiles of Central Florida Puerto Ricans: Examining the Influence of Skin Color, Perceived Ethnic-Racial Discrimination, and Neighborhood Ethnic-Racial Composition. Journal of Cross-Cultural Psychology, 50(4), 556576. https://doi.org/10.1177/0022022119835979 Enders, C. K. (2010). Applied missing data analysis. Guilford Press. Field, A. P. (2012). Discovering statistics using R. Sage. Hurtado, S. (2007). Linking Diversity with the Educational and Civic Missions of Higher Education. Review of Higher Education: Journal of the Association for the Study of Higher Education, 30(2), 185196. https://doi.org/10.1353/rhe.2006.0070 Hurtado, S., &amp; Carter, D. F. (1997). Effects of college transition and perceptions of the campus racial climate on Latino college students sense of belonging. Sociology of Education, 70, 324345. https://doi.org/10.2307/2673270 Iacovino, J. M., &amp; James, S. A. (2016). Retaining Students of Color in Higher Education: Expanding Our Focus to Psychosocial Adjustment and Mental Health. In The Crisis of Race in Higher Education: A Day of Discovery and Dialogue (Vol. 19, pp. 6184). Emerald Group Publishing Limited. https://doi.org/10.1108/S1479-364420160000019004 Kim, P. (2021). Yes, Asians and Asian Americans experience racism. In The Seattle Times. https://www.seattletimes.com/opinion/yes-asians-and-asian-americans-experience-racism/ Kim, P. Y. (2021). Guest Post: Anti-Asian Racism during the Pandemic: How Faculty on Christian Campuses Can Support Asian and Asian American Students. In Christian Scholars Review. https://christianscholars.com/guest-post-anti-asian-racism-during-the-pandemic-how-faculty-on-christian-campuses-can-support-asian-and-asian-american-students/ Kline, R. B. (2016). Principles and practice of structural equation modeling (4th ed.). Guilford Publications. http://ebookcentral.proquest.com/lib/spu/detail.action?docID=4000663 Lewis, K. R., &amp; Shah, P. P. (2019). Black students narratives of diversity and inclusion initiatives and the campus racial climate: An interest-convergence analysis. Journal of Diversity in Higher Education. https://doi.org/10.1037/dhe0000147 Little, R. J. A., &amp; Rubin, D. B. (2002). Statistical analysis with missing data (Second edition.). Wiley. http://site.ebrary.com/id/10921256 Mallinckrodt, B., Miles, J. R., &amp; Levy, J. J. (2014). The scientist-practitioner-advocate model: Addressing contemporary training needs for social justice advocacy. Training and Education in Professional Psychology, 8(4), 303311. https://doi.org/10.1037/tep0000045 Mosley, D. V., Hargons, C. N., Meiller, C., Angyal, B., Wheeler, P., Davis, C., &amp; Stevens-Watkins, D. (2021). Critical consciousness of anti-Black racism: A practical model to prevent and resist racial trauma. Journal of Counseling Psychology, 68(1), 116. https://doi.org/10.1037/cou0000430 Mosley, D. V., Neville, H. A., ChavezDueñas, N. Y., Adames, H. Y., Lewis, J. A., &amp; French, B. H. (2020). Radical hope in revolting times: Proposing a culturally relevant psychological framework. Social and Personality Psychology Compass, 14(1). https://doi.org/10.1111/spc3.12512 Parent, M. C. (2013). Handling item-level missing data: Simpler is just as good. The Counseling Psychologist, 41(4), 568600. https://doi.org/10.1177/0011000012445176 Singh, A. (2020). Building a Counseling Psychology of Liberation: The Path Behind Us, Under Us, and Before Us. The Counseling Psychologist, 48(8), 11091130. https://doi.org/10.1177/0011000020959007 STOP AAPI HATE. (n.d.). Retrieved March 19, 2021, from https://stopaapihate.org/ Szymanski, D. M., &amp; Bissonette, D. (2020). Perceptions of the LGBTQ College Campus Climate Scale: Development and Psychometric Evaluation. Journal of Homosexuality, 67(10), 14121428. https://doi.org/10.1080/00918369.2019.1591788 "]]
